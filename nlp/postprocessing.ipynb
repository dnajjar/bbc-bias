{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd779fe1-7a48-4d55-aa2c-1ce17a71f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ANNOTATION SCIRPT/FORMAT/PROCEDURE FROM:\n",
    "\n",
    "https://github.com/hollyjackson/casualty_mentions_nyt/tree/main\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12b36a-07f1-4bd5-bdf1-41193f131df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "import datetime as dt\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append('../nltk_data/')\n",
    "import string\n",
    "from nltk import collocations\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import RegexpParser\n",
    "from nltk.tree import *\n",
    "\n",
    "# spaCy imports\n",
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22a8bd-587b-4f52-a19a-984895a8e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "PALESTINE_IDENTIFIERS = [\"Palestine\", \"Palestinian\", \"Palestinians\"]\n",
    "ISRAEL_IDENTIFIERS = [\"Israel\", \"Israeli\", \"Israelis\"]\n",
    "\n",
    "# Cities in the West Bank and Gaza\n",
    "PALESTINIAN_CITIES = [\"Gaza\", \"Gaza Strip\", \"Jerusalem\", \"Abasan al-Kabira\", \"Abu Dis\", \"Bani Na\\'im\", \"Bani Suheila\",\n",
    "                      \"Beit Hanoun\", \"Beit Jala\", \"Beit Lahia\", \"Beit Sahour\", \"Beit Ummar\", \"Beitunia\", \"Bethlehem\",\n",
    "                      \"Beit Lahm\", \"al-Bireh\", \"Deir al-Balah\", \"ad-Dhahiriya\", \"Dura\", \"Gaza City\", \"Ghazzah\",\n",
    "                      \"Halhul\",\n",
    "                      \"Hebron\", \"al-Khalil\", \"Idhna\", \"Jabalia\", \"Jenin\", \"Jericho\", \"Ariha\", \"Khan Yunis\", \"Nablus\",\n",
    "                      \"Qabatiya\", \"Qalqilya\", \"Rafah\", \"Ramallah\", \"Sa\\'ir\", \"as-Samu\", \"Surif\", \"Tubas\", \"Tulkarm\",\n",
    "                      \"Ya\\'bad\", \"al-Yamun\", \"Yatta\", \"az-Zawayda\", \"Nazareth\", \"Jaljulia\", \"Kafr Bara\", \"Kafr Qasim\",\n",
    "                      \"Qalansawe\", \"Tayibe\", \"Tira\", \"Zemer\", \"Ar\\'ara\", \"Baqa al-Gharbiyye\", \"al-Arian\", \"Basma\",\n",
    "                      \"Jatt\",\n",
    "                      \"Kafr Qara\", \"Ma\\'ale Iron\", \"Meiser\", \"Umm al-Fahm\", \"Umm al-Qutuf\", \"Lod\", \"Ramla\",\n",
    "                      \"Wadi Nisnas\",\n",
    "                      \"Halisa\", \"Kababir\", \"Abbas\", \"Daliyat al-Karmel\", \"Ein Hawd\", \"Fureidis\", \"Ibtin\", \"Isfiya\",\n",
    "                      \"Jisr az-Zarqa\", \"Khawaled\", \"Abu Ghosh\", \"Beit Jimal\", \"Ein Naqquba\", \"Ein Rafa\", \"Beit Hanina\",\n",
    "                      \"Beit Safafa\", \"Jabel Mukaber\", \"Old City\", \"Ras al-Amud\", \"Sheikh Jarrah\", \"Shuafat\", \"Silwan\",\n",
    "                      \"Sur Baher\", \"At-Tur\", \"Umm Tuba\", \"Wadi al-Joz\", \"al-Walaja\", \"Abu Qrenat\", \"Abu Talul\",\n",
    "                      \"Ar\\'arat an-Naqab\", \"Ateer\", \"al-Atrash\", \"Bir Hadaj\", \"Dhahiyah\", \"Drijat\", \"Ghazzah\", \"Hura\",\n",
    "                      \"Kukhleh\", \"Kuseife\", \"Lakiya\", \"Makhul\", \"Mitnan\", \"Mulada\", \"Qasr al-Sir\", \"Rahat\", \"al-Sayyid\",\n",
    "                      \"Shaqib al-Salam\", \"Tirabin al-Sana\", \"Tel as-Sabi\", \"Umm Batin\", \"Abu Sinan\", \"Arab al-Aramshe\",\n",
    "                      \"Arab al-Subeih\", \"Arab al-Na\\'im\", \"Arraba\", \"Basmat Tab\\'un\", \"Beit Jann\", \"Bi\\'ina\",\n",
    "                      \"Bir al-Maksur\", \"Bu\\'eine Nujeidat\", \"Buqei\\'a\", \"Daburiyya\", \"Ed Dahi\", \"Deir al-Asad\",\n",
    "                      \"Deir Hanna\", \"Dmeide\", \"Eilabun\", \"Ein al-Asad\", \"Ein Mahil\", \"Fassuta\", \"Hamaam\", \"Hamdon\",\n",
    "                      \"Hurfeish\", \"Hussniyya\", \"I\\'billin\", \"Iksal\", \"Ilut\", \"Jadeidi-Makr\", \"Jish\", \"Julis\",\n",
    "                      \"Ka\\'abiyye-Tabbash-Hajajre\", \"Kabul\", \"Kafr Kanna\", \"Kafr Manda\", \"Kafr Misr\", \"Kafr Yasif\",\n",
    "                      \"Kamanneh\", \"Kaukab Abu al-Hija\", \"Kfar Kama\", \"Kisra-Sumei\", \"Maghar\", \"Majd al-Krum\",\n",
    "                      \"Manshiya Zabda\", \"Mashhad\", \"Mazra\\'a\", \"Mi\\'ilya\", \"Muqeible\", \"Nahf\", \"Na\\'ura\", \"Nazareth\",\n",
    "                      \"Nein\", \"Rameh\", \"Ras al-Ein\", \"Rehaniya\", \"Reineh\", \"Rumana\", \"Rumat al-Heib\", \"Sajur\",\n",
    "                      \"Sakhnin\",\n",
    "                      \"Sallama\", \"Sandala\", \"Sha\\'ab\", \"Shefa-\\'Amr\", \"Sheikh Danun\", \"Shibliâ€“Umm al-Ghanam\", \"Sulam\",\n",
    "                      \"Suweid Hamira\", \"Tarshiha\", \"Tamra City\", \"Tamra Village\", \"Tuba-Zangariyye\", \"Tur\\'an\", \"Uzeir\",\n",
    "                      \"Yafa an-Naseriyye\", \"Yanuh-Jat\", \"Yarka\", \"Zarzir\", \"Bani Suheila\", \"Beit Hanoun\", \"Beit Lahiya\",\n",
    "                      \"Deir al-Balah\", \"Jabalia\", \"Khan Yunis\", \"Rafah\"]\n",
    "\n",
    "# Cities in Israel ('48 lands)\n",
    "ISRAELI_CITIES = [\"Acre\", \"Afula\", \"Arad\", \"Arraba\", \"Ashdod\", \"Ashkelon\", \"Baqa al-Gharbiyye\", \"Bat Yam\", \"Beersheba\",\n",
    "                  \"Beit She\\'an\", \"Beit Shemesh\", \"Bnei Brak\", \"Dimona\", \"Eilat\", \"El\\'ad\", \"Giv\\'at Shmuel\",\n",
    "                  \"Givatayim\",\n",
    "                  \"Hadera\", \"Haifa\", \"Herzliya\", \"Hod HaSharon\", \"Holon\", \"Jerusalem\", \"Kafr Qasim\", \"Karmiel\",\n",
    "                  \"Kfar Saba\",\n",
    "                  \"Kfar Yona\", \"Kiryat Ata\", \"Kiryat Bialik\", \"Kiryat Gat\", \"Kiryat Malakhi\", \"Kiryat Motzkin\",\n",
    "                  \"Kiryat Ono\",\n",
    "                  \"Kiryat Shmona\", \"Kiryat Yam\", \"Lod\", \"Ma\\'alot-Tarshiha\", \"Migdal HaEmek\",\n",
    "                  \"Modi\\'in-Maccabim-Re\\'ut\",\n",
    "                  \"Nahariya\", \"Nazareth\", \"Nesher\", \"Ness Ziona\", \"Netanya\", \"Netivot\", \"Nof HaGalil\", \"Ofakim\",\n",
    "                  \"Or Akiva\",\n",
    "                  \"Or Yehuda\", \"Petah Tikva\", \"Qalansawe\", \"Ra\\'anana\", \"Rahat\", \"Ramat Gan\", \"Ramat HaSharon\", \"Ramla\",\n",
    "                  \"Rehovot\", \"Rishon LeZion\", \"Rosh HaAyin\", \"Safed\", \"Sakhnin\", \"Sderot\", \"Shefa-\\'Amr\", \"Tamra\",\n",
    "                  \"Tayibe\",\n",
    "                  \"Tel Aviv-Yafo\", \"Tel Aviv\", \"Tiberias\", \"Tira\", \"Tirat Carmel\", \"Umm al-Fahm\", \"Yavne\",\n",
    "                  \"Yehud-Monosson\",\n",
    "                  \"Yokneam Illit\"]\n",
    "\n",
    "PALESTINE_MEMBER_AFFILIATIONS = PALESTINE_IDENTIFIERS + PALESTINIAN_CITIES\n",
    "ISRAEL_MEMBER_AFFILIATIONS = ISRAEL_IDENTIFIERS + ISRAELI_CITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d809d8-fcf2-41ef-bdbb-3f139172cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# enumerate spacy subject types\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "\n",
    "# 27. VB  Verb, base form\n",
    "# 28. VBD Verb, past tense\n",
    "# 29. VBG Verb, gerund or present participle\n",
    "# 30. VBN Verb, past participle\n",
    "# 31. VBP Verb, non-3rd person singular present\n",
    "# 32. VBZ Verb, 3rd person singular present\n",
    "VERBS = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "ADJECTIVES = [\"JJ\"]\n",
    "\n",
    "# Enumerate common words related to death for automated sentence tagging\n",
    "FATAL_ADJECTIVES = [\"dead\", \"deceased\", \"buried\", \"killed\"]\n",
    "FATAL_VERBS_PASSIVE = [\"die\", \"decease\"]\n",
    "FATAL_VERBS_ACTIVE = [\"kill\", \"murder\", \"massacre\", \"shoot\", \"assassinate\", \"stab\", \"slash\"]\n",
    "FATAL_VERBS_ACTIVE_SPECIFIC = [\"behead\", \"slaughter\", \"execute\", \"hang\"]\n",
    "ALL_FATAL_VERBS = FATAL_VERBS_PASSIVE + FATAL_VERBS_ACTIVE + FATAL_VERBS_ACTIVE_SPECIFIC\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Choosing a dataset\n",
    "sample_size = 1\n",
    "\n",
    "# Defining the dataset path\n",
    "results_prefix = \"./results\"\n",
    "\n",
    "input_files = os.listdir(results_prefix + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22c061-d906-4cd9-999d-2f229af638bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_compound(dep_idx, dependencies_by_governor):\n",
    "    # Follow compound chain and return descriptors of a dependency\n",
    "    visited = set([dep_idx])\n",
    "    current_idx = dep_idx\n",
    "    descriptors = set()\n",
    "\n",
    "    found = True\n",
    "    while found:\n",
    "        found = False\n",
    "        for dep in dependencies_by_governor[current_idx]:\n",
    "            if dep[\"dep\"] == \"compound:prt\" or dep[\"dep\"] == \"compound\":\n",
    "                current_idx = dep[\"dependent\"]\n",
    "                descriptors.add(dep[\"dependentGloss\"])\n",
    "                if current_idx not in visited:\n",
    "                    found = True\n",
    "                    visited.add(current_idx)\n",
    "                break\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89336b-bbdd-45ee-a46f-554ddc4ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_subject(subject, dependencies_by_governor):\n",
    "    # Investigate all dependencies related to a subject to find as many descriptors as possible\n",
    "    # Present in a tiered list based on \"closeness\" to subject\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    subject_descriptors = [set(), set(), set(), 1]\n",
    "    subject_descriptors[0].add(subject[1])\n",
    "    # AMOD takes precedence over NMOD? takes precedence over ACL\n",
    "    for dep in dependencies_by_governor[subj_idx]:\n",
    "        if (dep[\"dep\"] == \"amod\"):\n",
    "            subject_descriptors[0].add(dep[\"dependentGloss\"])\n",
    "            if verbose:\n",
    "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
    "\n",
    "        if (dep[\"dep\"] == \"acl\" or dep[\"dep\"] == \"acl:relcl\"):\n",
    "            subject_descriptors[1].add(dep[\"dependentGloss\"])\n",
    "            # INVESTIGATE THE SUBJECT AND OBJECT OF DESCRIPTIVE CLAUSE\n",
    "            if verbose:\n",
    "                print(dep[\"dep\"], dep[\"dependentGloss\"])\n",
    "                print(dependencies_by_governor[dep[\"dependent\"]])\n",
    "                print()\n",
    "            for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
    "                # check nsubj and check obj\n",
    "                if (double_dep[\"dep\"] == \"nsubj\" or double_dep[\"dep\"] == \"nsubj:pass\" or double_dep[\n",
    "                    \"dep\"] == \"nsubj:outer\"\n",
    "                        or double_dep[\"dep\"] == \"csubj\" or double_dep[\"dep\"] == \"csubj:pass\" or double_dep[\n",
    "                            \"dep\"] == \"csubj:outer\"\n",
    "                        or double_dep[\"dep\"] == \"obj\"):\n",
    "                    subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
    "\n",
    "        if (dep[\"dep\"] == \"nmod\" or dep[\"dep\"] == \"nmod:npmod\" or dep[\"dep\"] == \"nmod:tmod\" or dep[\n",
    "            \"dep\"] == \"nmod:poss\"):\n",
    "            subject_descriptors[1].add(dep[\"dependentGloss\"])\n",
    "            if verbose:\n",
    "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
    "\n",
    "        if (dep[\"dep\"] == \"advmod\"):\n",
    "            subject_descriptors[1].add(dep[\"dependentGloss\"])  # TODO: CHECK THIS\n",
    "            if verbose:\n",
    "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
    "\n",
    "        if (dep[\"dep\"] == \"appos\"):\n",
    "            subject_descriptors[0].add(dep[\"dependentGloss\"])  # TODO:CHECK THIS\n",
    "            # Look for adjectives for appos also\n",
    "            # CHECK COMPOUD AND AMOD\n",
    "            to_add = follow_compound(dep[\"dependent\"], dependencies_by_governor)\n",
    "            if verbose:\n",
    "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\", to_add)\n",
    "            for thing in to_add:\n",
    "                subject_descriptors[0].add(thing)\n",
    "            for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
    "                if (double_dep[\"dep\"] == \"amod\"):\n",
    "                    subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
    "\n",
    "        #         if (dep[\"dep\"] == \"ccomp\"):\n",
    "        #             print(dependencies_by_governor[dep[\"dependent\"]])\n",
    "        #             for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
    "        #                 if (double_dep[\"dep\"] == \"nsubj\" or double_dep[\"dep\"] == \"nsubj:pass\"\n",
    "        #                      or double_dep[\"dep\"] == \"csubj\" or double_dep[\"dep\"] == \"csubj:pass\"):\n",
    "        #                     subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
    "\n",
    "        if (dep[\"dep\"] == \"nummod\"):\n",
    "            try:\n",
    "                subject_descriptors[3] = locale.atoi(dep[\"dependentGloss\"])\n",
    "            except:\n",
    "                subject_descriptors[3] = dep[\"dependentGloss\"]\n",
    "\n",
    "    return subject_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb36f6d-ac66-448d-a552-e5df545c9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(sentences):\n",
    "    # extract all sentences in an article\n",
    "    sentences_text = [None] * len(sentences)\n",
    "    for sentence in sentences:\n",
    "        sentence_index = sentence[\"index\"]\n",
    "\n",
    "        tokens = sentence[\"tokens\"]\n",
    "        sentence_text = \"\"\n",
    "        for token in tokens:\n",
    "            sentence_text += token[\"before\"] + token[\"word\"] + token[\"after\"]\n",
    "\n",
    "        sentences_text[sentence_index] = sentence_text\n",
    "    return sentences_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f259a4f-9fe1-4cf4-b2bf-61fe8b2e31f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def refresh_screen():\n",
    "    clear_output()\n",
    "    sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32eb405-6dff-4a9e-bcd8-c57e4f5416e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data for annotation\n",
    "df = pd.read_csv('./data/summary_20231201_livefeeds.csv\\')\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a0dc8-717e-4301-8fa0-5d2c1ed73b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = './'\n",
    "\n",
    "# list of ids we have already analysed\n",
    "analysed_ids = [int(f.split(\".\")[0].split(\"_\")[-1]) for f in os.listdir(root_directory + \"fatality_counts/\") if (f != '' and f not in ['.gitkeep','archive','summary'])]\n",
    "\n",
    "# init to handle duplicates\n",
    "assigned_sentences = {} \n",
    "\n",
    "to_annotate = df.shape[0]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    _ids = []\n",
    "    titles = []\n",
    "    dates = []\n",
    "    voices = []\n",
    "    categories = []\n",
    "    recorded_sentences = []\n",
    "    \n",
    "    results_file = row['results_file']\n",
    "    article_file = row['article_file']\n",
    "    _id = row['id']\n",
    "    \n",
    "    \n",
    "    if _id in analysed_ids:\n",
    "        continue\n",
    "\n",
    "    # Open NLP-analyzed result\n",
    "    filename = root_directory + results_file\n",
    "    try:\n",
    "        with open(filename) as d:\n",
    "            print(filename)\n",
    "            data = json.load(d)\n",
    "    except FileNotFoundError:\n",
    "        print('FILE NOT FOUND')\n",
    "        continue\n",
    "\n",
    "    # Open original text block from preprocessed data file\n",
    "    original_filename = root_directory + article_file\n",
    "    f = open(original_filename, \"r\")\n",
    "    article_text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Extract original date\n",
    "    title, date = row['title'], row['date'].date().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Extract NLP results\n",
    "    sentences = data[\"sentences\"]\n",
    "    text_all_sentences = extract_sentences(sentences)\n",
    "    \n",
    "    #for s in text_all_sentences:\n",
    "    #    print(s, '\\n')\n",
    "    \n",
    "    # Iterate through POS labels for each token\n",
    "    file_count = -1\n",
    "    fn_started = False\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        sentence_index = sentence[\"index\"]\n",
    "\n",
    "        tokens = sentence[\"tokens\"]\n",
    "        dependencies = sentence[\"basicDependencies\"]\n",
    "        sentence_text = \"\"\n",
    "        for token in tokens:\n",
    "            sentence_text += token[\"before\"] + token[\"word\"] + token[\"after\"]\n",
    "            \n",
    "        #print(sentence_text)\n",
    "\n",
    "        # create a data structure that maps governor dep_idx to dependencies\n",
    "        dependencies_by_governor = [[] for i in range(len(tokens) + 1)]\n",
    "        for dep in dependencies:\n",
    "            gov_idx = dep[\"governor\"]\n",
    "            dependencies_by_governor[gov_idx].append(dep)\n",
    "\n",
    "        # create data structure for tokens\n",
    "        tokens_by_idx = {}\n",
    "        for token in tokens:\n",
    "            tokens_by_idx[token[\"index\"]] = token\n",
    "\n",
    "        prepositional_information = None\n",
    "        for token in tokens:\n",
    "            pos = token[\"pos\"]\n",
    "            word = token[\"word\"]\n",
    "            lemma = token[\"lemma\"]\n",
    "            dep_idx = token[\"index\"]\n",
    "                \n",
    "                \n",
    "            # determine whether the sentence contains a fatal mention, whether through verbs or adjective\n",
    "            if pos in VERBS:\n",
    "                if lemma not in ALL_FATAL_VERBS:\n",
    "                    #print(\"VERB BUT NOT FATAL - SKIPPING.\")\n",
    "                    continue\n",
    "            else:\n",
    "                if pos in ADJECTIVES:\n",
    "                    if lemma not in FATAL_ADJECTIVES:\n",
    "                        #print(\"ADJ BUT NOT FATAL - SKIPPING.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    #print(\"NOT VERB OR ADJ - SKIPPING.\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "            verb_active = False if lemma in FATAL_VERBS_PASSIVE else True\n",
    "\n",
    "            # Find subject\n",
    "            voice = None\n",
    "            for dep in dependencies_by_governor[dep_idx]:\n",
    "                # If the sentence is in active voice, a 'nsubj' dependecy should exist.\n",
    "                # If the sentence is in passive voice a 'nsubjpass' dependency should exist\n",
    "                if dep[\"dep\"] == \"nsubj\" or dep[\"dep\"] == \"csubj\":\n",
    "                    voice = \"ACTIVE\"\n",
    "                    break\n",
    "                elif dep[\"dep\"] == \"nsubj:pass\" or dep[\"dep\"] == \"csubj:pass\" or dep[\"dep\"] == \"aux:pass\":\n",
    "                    voice = \"PASSIVE\"\n",
    "                    break\n",
    "\n",
    "            # for ACTIVE verbs, we are looking for dobj is used actively or nsubjpass if used passively\n",
    "            # for PASSIVE verbs, we are looking for nsubj if used actively or nsubjpass if used passively\n",
    "            subject = None\n",
    "            perp_keyword = None\n",
    "            for dep in dependencies_by_governor[dep_idx]:\n",
    "                if voice == \"ACTIVE\" and not verb_active and dep[\"dep\"] == \"nsubj\":\n",
    "                    # Example --> She died\n",
    "                    subject = (dep[\"dependent\"], dep[\"dependentGloss\"])\n",
    "                    break\n",
    "                if voice == \"PASSIVE\" and dep[\"dep\"] == \"nsubj:pass\":\n",
    "                    # Example --> She is deceased, She was killed\n",
    "                    subject = (dep[\"dependent\"], dep[\"dependentGloss\"])\n",
    "                    #                         perp_keyword = \"iobj\" NEED TO CHECK THIS\n",
    "                    break\n",
    "                if verb_active and dep[\"dep\"] == \"obj\":\n",
    "                    voice = \"ACTIVE\"\n",
    "                    # Example --> He killed her\n",
    "                    subject = (dep[\"dependent\"], dep[\"dependentGloss\"])\n",
    "                    #                         perp_keyword = \"nsubj\"\n",
    "                    break\n",
    "\n",
    "            # If subject found, find GUESS for subject's affiliation\n",
    "            IS_MEMBER_PALESTINE = False\n",
    "            IS_MEMBER_ISRAEL = False\n",
    "            if subject is not None:\n",
    "                subj_idx = subject[0]\n",
    "                subj_token = tokens_by_idx[subj_idx]\n",
    "                print(\"SUBJECT\", subject[1], subj_token[\"ner\"])\n",
    "                subject_descriptors = investigate_subject(subject, dependencies_by_governor)\n",
    "                # Check prepositional phrases for this sentence\n",
    "                if prepositional_information is None:\n",
    "                    prepositional_information = set()\n",
    "                    for dep in dependencies:\n",
    "                        if (dep[\"dep\"] == \"case\"):\n",
    "                            if (tokens_by_idx[dep[\"dependent\"]][\"lemma\"] == \"at\"\n",
    "                                    or tokens_by_idx[dep[\"dependent\"]][\"lemma\"] == \"in\"):\n",
    "                                prepositional_information.add(dep[\"governorGloss\"])\n",
    "                for pi in prepositional_information:\n",
    "                    subject_descriptors[2].add(pi)\n",
    "\n",
    "                # Finally, check prepositional phrases in the sentence\n",
    "                # token --> case --> to preposition (in or at)\n",
    "                ### ---------------TODO: MUST CHANGE HERE WHEN DATASET UPDATES\n",
    "                for j in range(3):\n",
    "                    for sd in subject_descriptors[j]:\n",
    "                        if sd in PALESTINE_MEMBER_AFFILIATIONS:\n",
    "                            IS_MEMBER_PALESTINE = True\n",
    "                        if sd in ISRAEL_MEMBER_AFFILIATIONS:\n",
    "                            IS_MEMBER_ISRAEL = True\n",
    "\n",
    "            # MANUALLY VALIDATE CATEGORY\n",
    "            if sentence_text not in assigned_sentences:\n",
    "                print(index, '/', to_annotate)\n",
    "                print('Please assign a category to the VICTIM:')\n",
    "                if (IS_MEMBER_PALESTINE or IS_MEMBER_ISRAEL) and not (IS_MEMBER_PALESTINE and IS_MEMBER_ISRAEL):\n",
    "                    category_guess = 'palestine' if IS_MEMBER_PALESTINE else 'israel'\n",
    "                    print('My guess is the VICTIM is from', category_guess)\n",
    "                assigned = False\n",
    "                sentence_chunk = \" \".join(\n",
    "                    text_all_sentences[max(0, sentence_index - 3):min(len(sentences) - 1, sentence_index + 4)])\n",
    "                for string in [sentence_text, sentence_chunk, article_text]:\n",
    "                    print(string)\n",
    "                    while True:\n",
    "                        mapper = {'pal':'palestine', 'is':'israel'}\n",
    "                        category = input(\n",
    "                            \"Enter category ('palestine', 'israel', 'both', none', or 'next' for the next string): \").strip().lower()\n",
    "                        print()\n",
    "                        if category in mapper:\n",
    "                            category = mapper[category]\n",
    "                        print('You chose', category)\n",
    "                        if category == 'palestine' or category == 'israel' or category == 'both':\n",
    "                            categories.append(category)\n",
    "                            assigned = True\n",
    "                            break\n",
    "                        elif category == 'none':\n",
    "                            assigned = True\n",
    "                            break\n",
    "                        elif category == 'next':\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid category. Please enter 'palestine', 'israel', 'both', 'none', or 'next'.\")\n",
    "                    if assigned:\n",
    "                        print('Categorized as', category)\n",
    "                        break\n",
    "\n",
    "                refresh_screen()\n",
    "                print()\n",
    "            else:\n",
    "                print('ASSIGNED TO EXISTING CATEGORY')\n",
    "                assigned = True\n",
    "                category = assigned_sentences[sentence_text]\n",
    "                categories.append(category)\n",
    "\n",
    "            if assigned and category != 'none':\n",
    "                # date_str = extract_date(index, file_count)\n",
    "                # date = parse_date(date_str)\n",
    "                dates.append(date)\n",
    "                voices.append(voice)\n",
    "                recorded_sentences.append(sentence_text)\n",
    "\n",
    "                # metadata\n",
    "                titles.append(title)\n",
    "                _ids.append(_id)\n",
    "\n",
    "                # add to dictionary to handle duplicates\n",
    "                assigned_sentences[sentence_text] = category\n",
    "                \n",
    "    # After all sentences are complete, save info for a file\n",
    "    file_dict = {\n",
    "        'article_id': _ids,\n",
    "        'article_title': titles,\n",
    "        'article_date': dates,\n",
    "        'sentence': recorded_sentences,\n",
    "        'category': categories,\n",
    "        'voice': voices\n",
    "    }\n",
    "    df = pd.DataFrame(file_dict)\n",
    "    df.to_csv(root_directory + \"fatality_counts/\" + 'articles_' + str(_id) + '.csv', index=False)\n",
    "    print('Saved CSV file, moving on to next article...', index + 1)\n",
    "    print('.......')\n",
    "    refresh_screen()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
